{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3123c593",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e80a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q pandas scikit-learn joblib tldextract matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa48a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9481f35",
   "metadata": {},
   "source": [
    "## 2. Upload Dataset\n",
    "\n",
    "Upload your training and validation CSV files:\n",
    "- `unified_ml_dataset_train.csv`\n",
    "- `unified_ml_dataset_val.csv`\n",
    "\n",
    "Or modify the paths below to load from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4858f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload files directly\n",
    "from google.colab import files\n",
    "print(\"Please upload unified_ml_dataset_train.csv:\")\n",
    "uploaded = files.upload()\n",
    "print(\"\\nPlease upload unified_ml_dataset_val.csv:\")\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67142ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Mount Google Drive (uncomment to use)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "# TRAIN_PATH = '/content/drive/MyDrive/datasets/unified_ml_dataset_train.csv'\n",
    "# VAL_PATH = '/content/drive/MyDrive/datasets/unified_ml_dataset_val.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54489aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths (modify if using Google Drive)\n",
    "TRAIN_PATH = 'unified_ml_dataset_train.csv'\n",
    "VAL_PATH = 'unified_ml_dataset_val.csv'\n",
    "OUTPUT_DIR = 'artifacts'\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "print(f\"✓ Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30e88d",
   "metadata": {},
   "source": [
    "## 3. Domain Reputation Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import math\n",
    "\n",
    "# Known URL shortener domains\n",
    "URL_SHORTENERS = {\n",
    "    'bit.ly', 'tinyurl.com', 'goo.gl', 'ow.ly', 't.co', 'is.gd', 'buff.ly',\n",
    "    'adf.ly', 'bit.do', 'mcaf.ee', 'su.pr', 'tiny.cc', 'tr.im', 'cli.gs',\n",
    "    'x.co', 'shorturl.at', 'cutt.ly', 'rb.gy', 'short.io', 'tiny.one',\n",
    "    'hyperurl.co', 'b2l.me', 'v.gd', 'lnkd.in', 'db.tt', 'qr.ae', 'bc.vc'\n",
    "}\n",
    "\n",
    "# Suspicious top-level domains\n",
    "SUSPICIOUS_TLDS = {\n",
    "    'xyz', 'top', 'club', 'work', 'click', 'link', 'online', 'site',\n",
    "    'website', 'space', 'tech', 'store', 'business', 'tk', 'ml', 'ga',\n",
    "    'cf', 'gq', 'pw', 'cc', 'info', 'ws', 'su', 'icu'\n",
    "}\n",
    "\n",
    "# Legitimate domains (whitelist)\n",
    "LEGITIMATE_DOMAINS = {\n",
    "    'google.com', 'youtube.com', 'facebook.com', 'amazon.com', 'wikipedia.org',\n",
    "    'twitter.com', 'instagram.com', 'linkedin.com', 'reddit.com', 'netflix.com',\n",
    "    'microsoft.com', 'apple.com', 'github.com', 'stackoverflow.com', 'medium.com',\n",
    "    'paypal.com', 'ebay.com', 'walmart.com', 'chase.com', 'bankofamerica.com',\n",
    "    'wellsfargo.com', 'citibank.com', 'usbank.com', 'capitalone.com',\n",
    "    'nytimes.com', 'cnn.com', 'bbc.com', 'bbc.co.uk', 'theguardian.com',\n",
    "    'washingtonpost.com', 'wsj.com', 'forbes.com', 'bloomberg.com',\n",
    "    'gmail.com', 'outlook.com', 'yahoo.com', 'hotmail.com', 'live.com',\n",
    "    'dropbox.com', 'docs.google.com', 'drive.google.com', 'icloud.com',\n",
    "    'adobe.com', 'salesforce.com', 'zoom.us', 'slack.com', 'shopify.com',\n",
    "    'spotify.com', 'twitch.tv', 'discord.com', 'telegram.org', 'whatsapp.com'\n",
    "}\n",
    "\n",
    "def extract_all_urls(text):\n",
    "    \"\"\"Extract all URLs from text.\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    url_pattern = r'https?://[^\\s]+|www\\.[^\\s]+|[a-zA-Z0-9-]+\\.[a-zA-Z]{2,}(?:/[^\\s]*)?'\n",
    "    urls = re.findall(url_pattern, str(text))\n",
    "    \n",
    "    normalized = []\n",
    "    for url in urls:\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            if url.startswith('www.'):\n",
    "                url = 'http://' + url\n",
    "            else:\n",
    "                url = 'http://' + url\n",
    "        normalized.append(url)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def is_ip_address(domain):\n",
    "    \"\"\"Check if domain is an IP address.\"\"\"\n",
    "    if not domain:\n",
    "        return False\n",
    "    ip_pattern = r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$'\n",
    "    return bool(re.match(ip_pattern, domain))\n",
    "\n",
    "def is_private_ip(ip_str):\n",
    "    \"\"\"Check if IP is in private range.\"\"\"\n",
    "    try:\n",
    "        parts = [int(p) for p in ip_str.split('.')]\n",
    "        if len(parts) != 4:\n",
    "            return False\n",
    "        if parts[0] == 10:\n",
    "            return True\n",
    "        if parts[0] == 172 and 16 <= parts[1] <= 31:\n",
    "            return True\n",
    "        if parts[0] == 192 and parts[1] == 168:\n",
    "            return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def has_non_standard_port(url):\n",
    "    \"\"\"Check if URL uses non-standard port.\"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        if parsed.port:\n",
    "            standard_ports = {80, 443, 8080}\n",
    "            if parsed.port not in standard_ports:\n",
    "                return True\n",
    "        port_pattern = r':(\\d+)'\n",
    "        match = re.search(port_pattern, url)\n",
    "        if match:\n",
    "            port = int(match.group(1))\n",
    "            return port not in {80, 443, 8080}\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def calculate_domain_entropy(domain):\n",
    "    \"\"\"Calculate Shannon entropy of domain.\"\"\"\n",
    "    if not domain:\n",
    "        return 0.0\n",
    "    entropy = 0.0\n",
    "    for char in set(domain):\n",
    "        p = domain.count(char) / len(domain)\n",
    "        entropy -= p * math.log2(p)\n",
    "    return entropy\n",
    "\n",
    "def extract_domain_features(text):\n",
    "    \"\"\"Extract domain reputation features.\"\"\"\n",
    "    features = {\n",
    "        'has_ip_url': 0,\n",
    "        'has_private_ip': 0,\n",
    "        'has_non_standard_port': 0,\n",
    "        'has_url_shortener': 0,\n",
    "        'has_suspicious_tld': 0,\n",
    "        'has_legitimate_domain': 0,\n",
    "        'domain_entropy': 0.0,\n",
    "        'subdomain_count': 0,\n",
    "        'url_path_length': 0,\n",
    "        'has_https': 0\n",
    "    }\n",
    "    \n",
    "    urls = extract_all_urls(str(text))\n",
    "    if not urls:\n",
    "        return features\n",
    "    \n",
    "    max_entropy = 0.0\n",
    "    max_subdomains = 0\n",
    "    max_path_length = 0\n",
    "    \n",
    "    for url in urls:\n",
    "        if url.startswith('https://'):\n",
    "            features['has_https'] = 1\n",
    "        \n",
    "        if has_non_standard_port(url):\n",
    "            features['has_non_standard_port'] = 1\n",
    "        \n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            netloc = parsed.netloc or parsed.path.split('/')[0]\n",
    "            netloc = netloc.split(':')[0]\n",
    "            \n",
    "            if is_ip_address(netloc):\n",
    "                features['has_ip_url'] = 1\n",
    "                if is_private_ip(netloc):\n",
    "                    features['has_private_ip'] = 1\n",
    "            \n",
    "            extracted = tldextract.extract(url)\n",
    "            domain = extracted.domain\n",
    "            tld = extracted.suffix\n",
    "            full_domain = f\"{domain}.{tld}\" if domain and tld else netloc\n",
    "            \n",
    "            if full_domain.lower() in URL_SHORTENERS:\n",
    "                features['has_url_shortener'] = 1\n",
    "            \n",
    "            if tld and tld.split('.')[-1].lower() in SUSPICIOUS_TLDS:\n",
    "                features['has_suspicious_tld'] = 1\n",
    "            \n",
    "            if full_domain.lower() in LEGITIMATE_DOMAINS:\n",
    "                features['has_legitimate_domain'] = 1\n",
    "            \n",
    "            entropy = calculate_domain_entropy(domain)\n",
    "            max_entropy = max(max_entropy, entropy)\n",
    "            \n",
    "            if extracted.subdomain:\n",
    "                subdomain_count = len(extracted.subdomain.split('.'))\n",
    "                max_subdomains = max(max_subdomains, subdomain_count)\n",
    "            \n",
    "            path = parsed.path\n",
    "            max_path_length = max(max_path_length, len(path))\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    features['domain_entropy'] = max_entropy\n",
    "    features['subdomain_count'] = max_subdomains\n",
    "    features['url_path_length'] = max_path_length\n",
    "    \n",
    "    return features\n",
    "\n",
    "def batch_extract_domain_features(texts):\n",
    "    \"\"\"Extract domain features for multiple texts.\"\"\"\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        results.append(extract_domain_features(text))\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"✓ Domain reputation feature extractor loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc87c85",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caab892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Clean text while preserving URL markers.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    t = str(text).lower()\n",
    "    t = re.sub(r'http\\S+', ' httpurl ', t)\n",
    "    t = re.sub(r'www\\.\\S+', ' wwwurl ', t)\n",
    "    t = re.sub(r'[^a-z0-9\\s]', ' ', t)\n",
    "    t = re.sub(r'\\s+', ' ', t).strip()\n",
    "    return t\n",
    "\n",
    "def extract_numeric_features(df, text_col='text'):\n",
    "    \"\"\"Extract basic + domain reputation numeric features.\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Basic features from unified dataset\n",
    "    features['url_count'] = df.get('url_count', 0).fillna(0)\n",
    "    features['text_length'] = df.get('text_length', df[text_col].str.len()).fillna(0)\n",
    "    features['word_count'] = df.get('word_count', df[text_col].str.split().str.len()).fillna(0)\n",
    "    features['special_char_ratio'] = df.get('special_char_ratio', 0).fillna(0)\n",
    "    features['digit_ratio'] = df.get('digit_ratio', 0).fillna(0)\n",
    "    features['uppercase_ratio'] = df.get('uppercase_ratio', 0).fillna(0)\n",
    "    features['suspicious_keywords'] = df.get('suspicious_keywords', 0).fillna(0)\n",
    "    \n",
    "    # Derived features\n",
    "    features['has_url'] = (features['url_count'] > 0).astype(int)\n",
    "    features['is_url_only'] = ((features['word_count'] <= 2) & (features['has_url'] == 1)).astype(int)\n",
    "    features['url_to_text_ratio'] = np.where(\n",
    "        features['text_length'] > 0,\n",
    "        features['url_count'] / (features['text_length'] / 50),\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Domain reputation features\n",
    "    print(\"  → Extracting domain reputation features...\")\n",
    "    domain_features = batch_extract_domain_features(df[text_col].values)\n",
    "    \n",
    "    # Add domain features with NaN handling\n",
    "    for col in domain_features.columns:\n",
    "        features[col] = domain_features[col].fillna(0)\n",
    "    \n",
    "    # Ensure no NaN/inf values\n",
    "    features = features.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"✓ Preprocessing functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aeadc3",
   "metadata": {},
   "source": [
    "## 5. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e6da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading training data...\")\n",
    "df_train = pd.read_csv(TRAIN_PATH, low_memory=False)\n",
    "print(f\"  Training samples: {len(df_train):,}\")\n",
    "\n",
    "print(\"\\nLoading validation data...\")\n",
    "df_val = pd.read_csv(VAL_PATH, low_memory=False)\n",
    "print(f\"  Validation samples: {len(df_val):,}\")\n",
    "\n",
    "# Clean data\n",
    "df_train = df_train.dropna(subset=['text', 'label']).copy()\n",
    "df_val = df_val.dropna(subset=['text', 'label']).copy()\n",
    "\n",
    "print(f\"\\nLabel distribution (training):\")\n",
    "print(df_train['label'].value_counts())\n",
    "\n",
    "print(f\"\\nSample texts:\")\n",
    "print(df_train[['text', 'label']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e870cc",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f95d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels\n",
    "y_train = (df_train['label'] == 'spam').astype(int)\n",
    "y_val = (df_val['label'] == 'spam').astype(int)\n",
    "\n",
    "# Preprocess text\n",
    "print(\"Preprocessing text...\")\n",
    "df_train['clean'] = df_train['text'].apply(preprocess)\n",
    "df_val['clean'] = df_val['text'].apply(preprocess)\n",
    "\n",
    "print(\"✓ Text preprocessing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b696178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TF-IDF features\n",
    "print(\"Extracting TF-IDF features...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=12000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5,\n",
    "    max_df=0.85,\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "X_train_text = vectorizer.fit_transform(df_train['clean'])\n",
    "X_val_text = vectorizer.transform(df_val['clean'])\n",
    "print(f\"  Vocabulary size: {len(vectorizer.vocabulary_):,}\")\n",
    "print(f\"  TF-IDF matrix shape: {X_train_text.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dad05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numeric features (this may take a few minutes)\n",
    "print(\"Extracting numeric + domain reputation features...\")\n",
    "X_train_numeric = extract_numeric_features(df_train, 'text')\n",
    "X_val_numeric = extract_numeric_features(df_val, 'text')\n",
    "\n",
    "print(f\"\\n  Total numeric features: {len(X_train_numeric.columns)}\")\n",
    "print(f\"  Feature list: {X_train_numeric.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5502bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numeric features\n",
    "print(\"Scaling numeric features...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_numeric_scaled = scaler.fit_transform(X_train_numeric)\n",
    "X_val_numeric_scaled = scaler.transform(X_val_numeric)\n",
    "print(\"✓ Scaling complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9845e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine features\n",
    "print(\"Combining TF-IDF + numeric + domain features...\")\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "X_train_combined = hstack([X_train_text, csr_matrix(X_train_numeric_scaled)])\n",
    "X_val_combined = hstack([X_val_text, csr_matrix(X_val_numeric_scaled)])\n",
    "\n",
    "print(f\"  Combined feature matrix shape: {X_train_combined.shape}\")\n",
    "print(f\"  Total features: {X_train_combined.shape[1]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b094239",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b418c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression with enhanced features...\")\n",
    "print(\"This may take 5-10 minutes...\\n\")\n",
    "\n",
    "model = LogisticRegression(\n",
    "    max_iter=300,\n",
    "    solver='saga',\n",
    "    class_weight={0: 1.0, 1: 0.8},  # Reduce false positives\n",
    "    C=3.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    penalty='l2',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(X_train_combined, y_train)\n",
    "print(\"\\n✓ Model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae16400",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a169793",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"VALIDATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "y_val_pred = model.predict(X_val_combined)\n",
    "y_val_proba = model.predict_proba(X_val_combined)[:, 1]\n",
    "\n",
    "report = classification_report(y_val, y_val_pred, target_names=['ham', 'spam'], digits=4)\n",
    "print(\"\\n\" + report)\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_val, y_val_pred, average='binary')\n",
    "roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "print(f\"\\nKey Metrics:\")\n",
    "print(f\"  ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"  F1 Score: {f1:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives:  {cm[0,0]:,}\")\n",
    "print(f\"  False Positives: {cm[0,1]:,}\")\n",
    "print(f\"  False Negatives: {cm[1,0]:,}\")\n",
    "print(f\"  True Positives:  {cm[1,1]:,}\")\n",
    "\n",
    "fp_rate = 100 * cm[0,1] / (cm[0,0] + cm[0,1])\n",
    "fn_rate = 100 * cm[1,0] / (cm[1,0] + cm[1,1])\n",
    "print(f\"\\n  False positive rate: {fp_rate:.2f}%\")\n",
    "print(f\"  False negative rate: {fn_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df1aa2",
   "metadata": {},
   "source": [
    "## 9. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22cea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Ham', 'Spam'], \n",
    "            yticklabels=['Ham', 'Spam'])\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(y_val, y_val_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fa941f",
   "metadata": {},
   "source": [
    "## 10. Save Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving model artifacts...\")\n",
    "\n",
    "# Save model, vectorizer, and scaler\n",
    "joblib.dump(model, f'{OUTPUT_DIR}/scam_detector_model_enhanced.joblib')\n",
    "joblib.dump(vectorizer, f'{OUTPUT_DIR}/scam_tfidf_vectorizer_enhanced.joblib')\n",
    "joblib.dump(scaler, f'{OUTPUT_DIR}/feature_scaler_enhanced.joblib')\n",
    "\n",
    "# Save feature config\n",
    "feature_config = {\n",
    "    'text_features': len(vectorizer.vocabulary_),\n",
    "    'numeric_features': X_train_numeric.columns.tolist(),\n",
    "    'total_features': int(X_train_combined.shape[1]),\n",
    "    'includes_domain_reputation': True,\n",
    "    'preprocessing': {\n",
    "        'url_marker': 'httpurl',\n",
    "        'www_marker': 'wwwurl'\n",
    "    }\n",
    "}\n",
    "with open(f'{OUTPUT_DIR}/feature_config_enhanced.json', 'w') as f:\n",
    "    json.dump(feature_config, f, indent=2)\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model_type': 'LogisticRegression + TF-IDF + Numeric + Domain Reputation',\n",
    "    'training_samples': int(len(df_train)),\n",
    "    'validation_samples': int(len(df_val)),\n",
    "    'vocabulary_size': int(len(vectorizer.vocabulary_)),\n",
    "    'total_features': int(X_train_combined.shape[1]),\n",
    "    'roc_auc': float(roc_auc),\n",
    "    'f1_score': float(f1),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'confusion_matrix': {\n",
    "        'tn': int(cm[0,0]),\n",
    "        'fp': int(cm[0,1]),\n",
    "        'fn': int(cm[1,0]),\n",
    "        'tp': int(cm[1,1])\n",
    "    }\n",
    "}\n",
    "with open(f'{OUTPUT_DIR}/metrics_enhanced.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Artifacts saved:\")\n",
    "print(f\"  - scam_detector_model_enhanced.joblib\")\n",
    "print(f\"  - scam_tfidf_vectorizer_enhanced.joblib\")\n",
    "print(f\"  - feature_scaler_enhanced.joblib\")\n",
    "print(f\"  - feature_config_enhanced.json\")\n",
    "print(f\"  - metrics_enhanced.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5aa15f",
   "metadata": {},
   "source": [
    "## 11. Download Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b254fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all artifacts\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"Downloading model artifacts...\")\n",
    "for filename in os.listdir(OUTPUT_DIR):\n",
    "    if filename.endswith(('.joblib', '.json')):\n",
    "        filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "        files.download(filepath)\n",
    "        print(f\"  ✓ Downloaded: {filename}\")\n",
    "\n",
    "print(\"\\n✓ All artifacts downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1672d",
   "metadata": {},
   "source": [
    "## 12. Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e48587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_message(text):\n",
    "    \"\"\"Predict if a message is spam or ham.\"\"\"\n",
    "    # Preprocess\n",
    "    clean_text = preprocess(text)\n",
    "    \n",
    "    # Extract TF-IDF features\n",
    "    X_text = vectorizer.transform([clean_text])\n",
    "    \n",
    "    # Extract numeric features\n",
    "    df_temp = pd.DataFrame({'text': [text]})\n",
    "    X_numeric = extract_numeric_features(df_temp, 'text')\n",
    "    X_numeric_scaled = scaler.transform(X_numeric)\n",
    "    \n",
    "    # Combine\n",
    "    X_combined = hstack([X_text, csr_matrix(X_numeric_scaled)])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(X_combined)[0]\n",
    "    probability = model.predict_proba(X_combined)[0, 1]\n",
    "    \n",
    "    label = 'SPAM' if prediction == 1 else 'HAM'\n",
    "    confidence = probability if prediction == 1 else (1 - probability)\n",
    "    \n",
    "    return label, confidence\n",
    "\n",
    "# Test examples\n",
    "test_messages = [\n",
    "    \"Congratulations! You've won $1000. Click here to claim: http://bit.ly/xyz\",\n",
    "    \"Hi John, I've shared the quarterly report. Let me know if you have questions.\",\n",
    "    \"URGENT: Your account has been suspended. Verify now: http://192.168.1.1/verify\",\n",
    "    \"Meeting at 3pm tomorrow in conference room B\",\n",
    "    \"Free iPhone! Limited time offer: http://get-free-phone.tk\"\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TEST PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, msg in enumerate(test_messages, 1):\n",
    "    label, conf = predict_message(msg)\n",
    "    print(f\"\\n{i}. {msg[:70]}...\")\n",
    "    print(f\"   Prediction: {label} (confidence: {conf:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d3802",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Model Performance\n",
    "- **ROC-AUC:** 99.83%\n",
    "- **Precision:** 99.77%\n",
    "- **Recall:** 98.36%\n",
    "- **False Positive Rate:** 0.23%\n",
    "\n",
    "### Features (12,020 total)\n",
    "1. **TF-IDF Features:** 12,000 text features\n",
    "2. **Basic Numeric:** 11 features (url_count, text_length, word_count, etc.)\n",
    "3. **Domain Reputation:** 10 features\n",
    "   - IP detection (has_ip_url, has_private_ip)\n",
    "   - URL shorteners (has_url_shortener)\n",
    "   - Suspicious TLDs (has_suspicious_tld)\n",
    "   - Non-standard ports (has_non_standard_port)\n",
    "   - Domain entropy, subdomain count, path length\n",
    "   - HTTPS detection\n",
    "\n",
    "### Next Steps\n",
    "1. Download the model artifacts\n",
    "2. Integrate into your application\n",
    "3. Deploy using FastAPI or Flask\n",
    "4. Monitor performance in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a30cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect domain features for test messages\n",
    "for i, msg in enumerate(test_messages, 1):\n",
    "    df_temp = pd.DataFrame({'text': [msg]})\n",
    "    domain_feats = batch_extract_domain_features(df_temp['text'])\n",
    "    print(f\"\\n{i}. {msg[:70]}...\")\n",
    "    print(domain_feats.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375845f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze false negatives: spam messages predicted as ham\n",
    "false_negatives = df_val[(y_val == 1) & (y_val_pred == 0)].copy()\n",
    "print(f\"False negatives (missed spam): {len(false_negatives):,}\")\n",
    "\n",
    "# Show a sample of missed spam messages\n",
    "sample_fn = false_negatives.sample(n=min(10, len(false_negatives)), random_state=42) if len(false_negatives) > 0 else false_negatives\n",
    "for idx, row in sample_fn.iterrows():\n",
    "    print(f\"\\nMessage: {row['text'][:100]}...\")\n",
    "    domain_feats = batch_extract_domain_features([row['text']])\n",
    "    print(\"Domain features:\")\n",
    "    print(domain_feats.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display false negatives in a DataFrame for easier inspection\n",
    "import IPython.display as disp\n",
    "\n",
    "if len(false_negatives) == 0:\n",
    "    print(\"No false negatives (missed spam) found!\")\n",
    "else:\n",
    "    # Show up to 20 missed spam messages with their domain features\n",
    "    sample_fn = false_negatives.sample(n=min(20, len(false_negatives)), random_state=42)\n",
    "    domain_feats = batch_extract_domain_features(sample_fn['text'])\n",
    "    df_display = sample_fn[['text']].copy()\n",
    "    df_display = df_display.reset_index(drop=True)\n",
    "    domain_feats = domain_feats.reset_index(drop=True)\n",
    "    df_display = pd.concat([df_display, domain_feats], axis=1)\n",
    "    disp.display(df_display)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
